{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tinytrainer_en.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"2d5ec94a2b864a05b669fd1da24024ac":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_66424ea69ffb4818aa195f8590fac332","IPY_MODEL_b1a766ea720f48278162f6fb4b0d8ff6","IPY_MODEL_9b9246795d8f48acb336af9f6812a23a","IPY_MODEL_1e2446db00094066be39859c9f6ec7d7"],"layout":"IPY_MODEL_f1bb3c78149843c7a78db0d9cee31e99"}},"66424ea69ffb4818aa195f8590fac332":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_562c3e2d0f0e4384959c4b453520fb06","placeholder":"​","style":"IPY_MODEL_b5a1db697cc34d3a9e254925c1fceff4","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"b1a766ea720f48278162f6fb4b0d8ff6":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_455d988a5e4744658eef15acedf8f51e","placeholder":"​","style":"IPY_MODEL_1f083dcfc59144dcb49180700cc72d8c","value":""}},"9b9246795d8f48acb336af9f6812a23a":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_8bd0e15ffd204e36af021f89ec92ad9e","style":"IPY_MODEL_d5a6da04dd2d4e1eb1f7153329fdd633","tooltip":""}},"1e2446db00094066be39859c9f6ec7d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6542aab132594e1f975bd662d360aa00","placeholder":"​","style":"IPY_MODEL_e321c1f7cc194e47bf1eead22dc97f4d","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"f1bb3c78149843c7a78db0d9cee31e99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"562c3e2d0f0e4384959c4b453520fb06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5a1db697cc34d3a9e254925c1fceff4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"455d988a5e4744658eef15acedf8f51e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f083dcfc59144dcb49180700cc72d8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bd0e15ffd204e36af021f89ec92ad9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5a6da04dd2d4e1eb1f7153329fdd633":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"6542aab132594e1f975bd662d360aa00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e321c1f7cc194e47bf1eead22dc97f4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install datasets\n","!pip install transformers\n","!pip install huggingface_hub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IdU-LlrN8z6S","executionInfo":{"status":"ok","timestamp":1659468267096,"user_tz":-120,"elapsed":18734,"user":{"displayName":"Andrés Langoyo Martín","userId":"12914381003507187562"}},"outputId":"4829de02-611f-49e0-c04d-66cacc5de825"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 5.0 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 14.3 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 51.4 MB/s \n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 60.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting fsspec[http]>=2021.11.1\n","  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n","\u001b[K     |████████████████████████████████| 141 kB 67.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 14.9 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 73.8 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, pyyaml, fsspec, xxhash, responses, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed datasets-2.4.0 fsspec-2022.7.1 huggingface-hub-0.8.1 multiprocess-0.70.13 pyyaml-6.0 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 50.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.12.1 transformers-4.21.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.7/dist-packages (0.8.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.1.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.64.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.12.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2022.6.15)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('./montura')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RdIoDOkLkrn","executionInfo":{"status":"ok","timestamp":1659468289172,"user_tz":-120,"elapsed":22088,"user":{"displayName":"Andrés Langoyo Martín","userId":"12914381003507187562"}},"outputId":"6bb7253c-0723-4f88-935f-f2a7ac691ccc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at ./montura\n"]}]},{"cell_type":"code","source":["!cd 'montura/MyDrive/gatech/TFG/minbert'\n","!ls './montura/MyDrive/gatech/TFG/minbert/bert_mini'\n","!git config --global user.name \"srcocotero\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ksj1Fh1zO9O8","executionInfo":{"status":"ok","timestamp":1659468289762,"user_tz":-120,"elapsed":609,"user":{"displayName":"Andrés Langoyo Martín","userId":"12914381003507187562"}},"outputId":"9c0d8302-cac5-42ca-adcc-cc331c06542a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 0: cd: montura/MyDrive/gatech/TFG/minbert: No such file or directory\n","ls: cannot access './montura/MyDrive/gatech/TFG/minbert/bert_mini': No such file or directory\n"]}]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","'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'\n","notebook_login()"],"metadata":{"id":"Tm82TkaCb8Tm","executionInfo":{"status":"ok","timestamp":1659468289765,"user_tz":-120,"elapsed":52,"user":{"displayName":"Andrés Langoyo Martín","userId":"12914381003507187562"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2d5ec94a2b864a05b669fd1da24024ac","66424ea69ffb4818aa195f8590fac332","b1a766ea720f48278162f6fb4b0d8ff6","9b9246795d8f48acb336af9f6812a23a","1e2446db00094066be39859c9f6ec7d7","f1bb3c78149843c7a78db0d9cee31e99","562c3e2d0f0e4384959c4b453520fb06","b5a1db697cc34d3a9e254925c1fceff4","455d988a5e4744658eef15acedf8f51e","1f083dcfc59144dcb49180700cc72d8c","8bd0e15ffd204e36af021f89ec92ad9e","d5a6da04dd2d4e1eb1f7153329fdd633","6542aab132594e1f975bd662d360aa00","e321c1f7cc194e47bf1eead22dc97f4d"]},"outputId":"09a925d2-0a5a-4f01-8f6e-8c1497447cb0"},"execution_count":4,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/huggingface_hub/commands/user.py\u001b[0m in \u001b[0;36mlogin_token_event\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mtoken_widget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0m_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mtoken_finish_button\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_click\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogin_token_event\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/huggingface_hub/commands/user.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(hf_api, token)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_or_retrieve_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0mhf_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_access_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0mHfFolder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m_validate_or_retrieve_token\u001b[0;34m(self, token, name, function_name)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must use your personal account token.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid token passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Invalid token passed!"]}]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","console.log(\"Working\");\n","document.querySelector(\"colab-toolbar-button#connect\").click()\n","}setInterval(ClickConnect,60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"X8GHb3NwOVu8","executionInfo":{"status":"ok","timestamp":1659468289767,"user_tz":-120,"elapsed":38,"user":{"displayName":"Andrés Langoyo Martín","userId":"12914381003507187562"}},"outputId":"2a3dfc3e-52a5-40d7-a223-ca0c3d26516a"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","console.log(\"Working\");\n","document.querySelector(\"colab-toolbar-button#connect\").click()\n","}setInterval(ClickConnect,60000)"]},"metadata":{}}]},{"cell_type":"code","source":["!python montura/MyDrive/TFG/run_qa.py \\\n","  --model_name_or_path nreimers/BERT-Tiny_L-2_H-128_A-2 \\\n","  --dataset_name squad \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 5 \\\n","  --learning_rate 3e-5 \\\n","  --num_train_epochs 3 \\\n","  --max_seq_length 512 \\\n","  --doc_stride 128 \\\n","  --output_dir ./montura/MyDrive/TFG/minibert/mini_model \\\n","  --push_to_hub False \\\n","  --hub_model_id srcocotero/tiny-bert-qa \\\n","  --overwrite_output_dir True"],"metadata":{"id":"Oy-zyaVQOc9L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659469390741,"user_tz":-120,"elapsed":1101005,"user":{"displayName":"Andrés Langoyo Martín","userId":"12914381003507187562"}},"outputId":"9e0d85d3-caf4-4e46-bb9f-34c4084110b3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["08/02/2022 19:24:56 - WARNING - mylogger - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","08/02/2022 19:24:56 - INFO - mylogger - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=srcocotero/tiny-bert-qa,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=./montura/MyDrive/TFG/minibert/mini_model/runs/Aug02_19-24-56_d316153addeb,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_hf,\n","output_dir=./montura/MyDrive/TFG/minibert/mini_model,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=5,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=./montura/MyDrive/TFG/minibert/mini_model,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","08/02/2022 19:24:57 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.4.0/datasets/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp6f6hs5_j\n","Downloading builder script: 5.27kB [00:00, 5.75MB/s]       \n","08/02/2022 19:24:57 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.4.0/datasets/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/b68d5e6fb5b9ae7d4239e1a3342f1c6947319004d70d95a87f402c2a997eebc9.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n","08/02/2022 19:24:57 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b68d5e6fb5b9ae7d4239e1a3342f1c6947319004d70d95a87f402c2a997eebc9.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n","08/02/2022 19:24:57 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.4.0/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpii7dkabw\n","Downloading metadata: 2.36kB [00:00, 3.63MB/s]       \n","08/02/2022 19:24:57 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.4.0/datasets/squad/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/780e6a8b8904a4864f4668eca5185f2cdcfd6d49bccfb3b937aba0ac9e0ce3d2.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n","08/02/2022 19:24:57 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/780e6a8b8904a4864f4668eca5185f2cdcfd6d49bccfb3b937aba0ac9e0ce3d2.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n","08/02/2022 19:24:57 - INFO - datasets.builder - No config specified, defaulting to the single config: squad/plain_text\n","08/02/2022 19:24:57 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n","08/02/2022 19:24:57 - INFO - datasets.builder - Generating dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n","Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n","08/02/2022 19:24:57 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]08/02/2022 19:24:58 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpid7_q2wf\n","\n","Downloading data:   0% 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  91% 7.39M/8.12M [00:00<00:00, 73.9MB/s]\u001b[A\n","Downloading data: 16.4MB [00:00, 83.6MB/s]                \u001b[A\n","Downloading data: 30.3MB [00:00, 85.9MB/s]\n","08/02/2022 19:24:58 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n","08/02/2022 19:24:58 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.18s/it]08/02/2022 19:24:58 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpe12vklzg\n","\n","Downloading data: 4.85MB [00:00, 75.2MB/s]       \n","08/02/2022 19:24:58 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n","08/02/2022 19:24:58 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n","Downloading data files: 100% 2/2 [00:01<00:00,  1.36it/s]\n","08/02/2022 19:24:58 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","08/02/2022 19:24:59 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1604.25it/s]\n","08/02/2022 19:24:59 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","08/02/2022 19:24:59 - INFO - datasets.builder - Generating train split\n","08/02/2022 19:25:07 - INFO - datasets.builder - Generating validation split\n","08/02/2022 19:25:08 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 309.62it/s]\n","[INFO|hub.py:600] 2022-08-02 19:25:08,406 >> https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpw1oqa8kz\n","Downloading config.json: 100% 479/479 [00:00<00:00, 560kB/s]\n","[INFO|hub.py:613] 2022-08-02 19:25:08,572 >> storing https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/1fc0ce00359c91b870a93e8ba98a005e2cfd49c171fdeadfc2ced617e6bcd965.aeebf9253ede7f07c1908a84771c6ad1924dedf7715087939765eb185df5252e\n","[INFO|hub.py:621] 2022-08-02 19:25:08,572 >> creating metadata file for /root/.cache/huggingface/transformers/1fc0ce00359c91b870a93e8ba98a005e2cfd49c171fdeadfc2ced617e6bcd965.aeebf9253ede7f07c1908a84771c6ad1924dedf7715087939765eb185df5252e\n","[INFO|configuration_utils.py:674] 2022-08-02 19:25:08,572 >> loading configuration file https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1fc0ce00359c91b870a93e8ba98a005e2cfd49c171fdeadfc2ced617e6bcd965.aeebf9253ede7f07c1908a84771c6ad1924dedf7715087939765eb185df5252e\n","[INFO|configuration_utils.py:723] 2022-08-02 19:25:08,575 >> Model config BertConfig {\n","  \"_name_or_path\": \"nreimers/BERT-Tiny_L-2_H-128_A-2\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 128,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 512,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 2,\n","  \"num_hidden_layers\": 2,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|hub.py:600] 2022-08-02 19:25:08,709 >> https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi64njlyr\n","Downloading tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 58.4kB/s]\n","[INFO|hub.py:613] 2022-08-02 19:25:08,846 >> storing https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/d68ac0f25262dc169e13ccd89842696676263390bd1cb0148f4559459ff4a7ed.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed\n","[INFO|hub.py:621] 2022-08-02 19:25:08,846 >> creating metadata file for /root/.cache/huggingface/transformers/d68ac0f25262dc169e13ccd89842696676263390bd1cb0148f4559459ff4a7ed.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed\n","[INFO|configuration_utils.py:674] 2022-08-02 19:25:08,982 >> loading configuration file https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1fc0ce00359c91b870a93e8ba98a005e2cfd49c171fdeadfc2ced617e6bcd965.aeebf9253ede7f07c1908a84771c6ad1924dedf7715087939765eb185df5252e\n","[INFO|configuration_utils.py:723] 2022-08-02 19:25:08,982 >> Model config BertConfig {\n","  \"_name_or_path\": \"nreimers/BERT-Tiny_L-2_H-128_A-2\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 128,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 512,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 2,\n","  \"num_hidden_layers\": 2,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|hub.py:600] 2022-08-02 19:25:09,253 >> https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2i2020cy\n","Downloading vocab.txt: 100% 226k/226k [00:00<00:00, 1.75MB/s]\n","[INFO|hub.py:613] 2022-08-02 19:25:09,533 >> storing https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/d9d25905f2cff0955fa193e426881ccf877554cce53eabfe39b28cdaa1ea712e.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","[INFO|hub.py:621] 2022-08-02 19:25:09,533 >> creating metadata file for /root/.cache/huggingface/transformers/d9d25905f2cff0955fa193e426881ccf877554cce53eabfe39b28cdaa1ea712e.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","[INFO|hub.py:600] 2022-08-02 19:25:09,953 >> https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpla4nbco2\n","Downloading special_tokens_map.json: 100% 112/112 [00:00<00:00, 143kB/s]\n","[INFO|hub.py:613] 2022-08-02 19:25:10,089 >> storing https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/448f85f42d7f87f0254da1997bc5cd60cb4607800084132993017232e82432a3.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n","[INFO|hub.py:621] 2022-08-02 19:25:10,089 >> creating metadata file for /root/.cache/huggingface/transformers/448f85f42d7f87f0254da1997bc5cd60cb4607800084132993017232e82432a3.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n","[INFO|tokenization_utils_base.py:1803] 2022-08-02 19:25:10,217 >> loading file https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/d9d25905f2cff0955fa193e426881ccf877554cce53eabfe39b28cdaa1ea712e.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","[INFO|tokenization_utils_base.py:1803] 2022-08-02 19:25:10,217 >> loading file https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1803] 2022-08-02 19:25:10,217 >> loading file https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1803] 2022-08-02 19:25:10,217 >> loading file https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/448f85f42d7f87f0254da1997bc5cd60cb4607800084132993017232e82432a3.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n","[INFO|tokenization_utils_base.py:1803] 2022-08-02 19:25:10,218 >> loading file https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/d68ac0f25262dc169e13ccd89842696676263390bd1cb0148f4559459ff4a7ed.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed\n","[INFO|configuration_utils.py:674] 2022-08-02 19:25:10,359 >> loading configuration file https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1fc0ce00359c91b870a93e8ba98a005e2cfd49c171fdeadfc2ced617e6bcd965.aeebf9253ede7f07c1908a84771c6ad1924dedf7715087939765eb185df5252e\n","[INFO|configuration_utils.py:723] 2022-08-02 19:25:10,360 >> Model config BertConfig {\n","  \"_name_or_path\": \"nreimers/BERT-Tiny_L-2_H-128_A-2\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 128,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 512,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 2,\n","  \"num_hidden_layers\": 2,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|configuration_utils.py:674] 2022-08-02 19:25:10,524 >> loading configuration file https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1fc0ce00359c91b870a93e8ba98a005e2cfd49c171fdeadfc2ced617e6bcd965.aeebf9253ede7f07c1908a84771c6ad1924dedf7715087939765eb185df5252e\n","[INFO|configuration_utils.py:723] 2022-08-02 19:25:10,525 >> Model config BertConfig {\n","  \"_name_or_path\": \"nreimers/BERT-Tiny_L-2_H-128_A-2\",\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 128,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 512,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 2,\n","  \"num_hidden_layers\": 2,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|hub.py:600] 2022-08-02 19:25:10,704 >> https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6i740ql1\n","Downloading pytorch_model.bin: 100% 16.9M/16.9M [00:00<00:00, 34.3MB/s]\n","[INFO|hub.py:613] 2022-08-02 19:25:11,495 >> storing https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/cdb36d3a76e0a8178f9eba06c8edcf66f02d9f290a762214b016ae207c275ec1.bbc4f13901c42773e09f470f393259b25f0af3d1a7444d2a742561f4d2423b15\n","[INFO|hub.py:621] 2022-08-02 19:25:11,495 >> creating metadata file for /root/.cache/huggingface/transformers/cdb36d3a76e0a8178f9eba06c8edcf66f02d9f290a762214b016ae207c275ec1.bbc4f13901c42773e09f470f393259b25f0af3d1a7444d2a742561f4d2423b15\n","[INFO|modeling_utils.py:2034] 2022-08-02 19:25:11,495 >> loading weights file https://huggingface.co/nreimers/BERT-Tiny_L-2_H-128_A-2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/cdb36d3a76e0a8178f9eba06c8edcf66f02d9f290a762214b016ae207c275ec1.bbc4f13901c42773e09f470f393259b25f0af3d1a7444d2a742561f4d2423b15\n","[WARNING|modeling_utils.py:2419] 2022-08-02 19:25:11,666 >> Some weights of the model checkpoint at nreimers/BERT-Tiny_L-2_H-128_A-2 were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2431] 2022-08-02 19:25:11,666 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at nreimers/BERT-Tiny_L-2_H-128_A-2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/88 [00:00<?, ?ba/s]08/02/2022 19:25:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-86009f978a13ea08.arrow\n","Running tokenizer on train dataset: 100% 88/88 [00:58<00:00,  1.49ba/s]\n","Running tokenizer on validation dataset:   0% 0/11 [00:00<?, ?ba/s]08/02/2022 19:26:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-3a903e3b86593464.arrow\n","Running tokenizer on validation dataset: 100% 11/11 [01:08<00:00,  6.27s/ba]\n","08/02/2022 19:27:19 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.4.0/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpi6x9avqo\n","Downloading builder script: 4.50kB [00:00, 5.25MB/s]       \n","08/02/2022 19:27:20 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.4.0/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/2e81e82f30b51acea650bf442664161a3d325ba6cbc452aadd2b9a666b5f3fa0.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n","08/02/2022 19:27:20 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/2e81e82f30b51acea650bf442664161a3d325ba6cbc452aadd2b9a666b5f3fa0.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n","08/02/2022 19:27:20 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.4.0/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp097qofxl\n","Downloading extra modules: 3.31kB [00:00, 4.23MB/s]       \n","08/02/2022 19:27:20 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.4.0/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/56ac7ec14cac1e0ea03a134379fda376641e8bc5cd49f767210f284ab8f99d52.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n","08/02/2022 19:27:20 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/56ac7ec14cac1e0ea03a134379fda376641e8bc5cd49f767210f284ab8f99d52.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1605] 2022-08-02 19:27:27,229 >> ***** Running training *****\n","[INFO|trainer.py:1606] 2022-08-02 19:27:27,229 >>   Num examples = 87714\n","[INFO|trainer.py:1607] 2022-08-02 19:27:27,229 >>   Num Epochs = 3\n","[INFO|trainer.py:1608] 2022-08-02 19:27:27,229 >>   Instantaneous batch size per device = 5\n","[INFO|trainer.py:1609] 2022-08-02 19:27:27,229 >>   Total train batch size (w. parallel, distributed & accumulation) = 5\n","[INFO|trainer.py:1610] 2022-08-02 19:27:27,229 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1611] 2022-08-02 19:27:27,229 >>   Total optimization steps = 52629\n","{'loss': 4.8185, 'learning_rate': 2.9714986034315684e-05, 'epoch': 0.03}\n","  1% 500/52629 [00:10<13:17, 65.33it/s][INFO|trainer.py:2640] 2022-08-02 19:27:37,912 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:27:37,917 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:27:37,991 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:27:37,995 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:27:38,006 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-500/special_tokens_map.json\n","{'loss': 4.2601, 'learning_rate': 2.942997206863136e-05, 'epoch': 0.06}\n","  2% 1000/52629 [00:18<13:42, 62.77it/s][INFO|trainer.py:2640] 2022-08-02 19:27:45,989 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-1000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:27:45,994 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:27:46,053 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:27:46,057 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:27:46,061 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-1000/special_tokens_map.json\n","{'loss': 4.1555, 'learning_rate': 2.9144958102947044e-05, 'epoch': 0.09}\n","  3% 1500/52629 [00:26<13:00, 65.47it/s][INFO|trainer.py:2640] 2022-08-02 19:27:53,993 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-1500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:27:53,999 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:27:54,066 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:27:54,071 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:27:54,075 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-1500/special_tokens_map.json\n","{'loss': 3.9985, 'learning_rate': 2.8859944137262728e-05, 'epoch': 0.11}\n","  4% 2000/52629 [00:34<13:25, 62.83it/s][INFO|trainer.py:2640] 2022-08-02 19:28:02,291 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-2000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:28:02,296 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:28:02,351 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:28:02,355 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:28:02,374 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-2000/special_tokens_map.json\n","{'loss': 3.9299, 'learning_rate': 2.857493017157841e-05, 'epoch': 0.14}\n","  5% 2500/52629 [00:42<12:42, 65.71it/s][INFO|trainer.py:2640] 2022-08-02 19:28:10,305 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-2500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:28:10,310 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:28:10,391 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:28:10,399 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:28:10,404 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-2500/special_tokens_map.json\n","{'loss': 3.8222, 'learning_rate': 2.8289916205894088e-05, 'epoch': 0.17}\n","  6% 3000/52629 [00:50<12:28, 66.34it/s][INFO|trainer.py:2640] 2022-08-02 19:28:18,401 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-3000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:28:18,407 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:28:18,468 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:28:18,472 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:28:18,476 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-3000/special_tokens_map.json\n","{'loss': 3.7104, 'learning_rate': 2.800490224020977e-05, 'epoch': 0.2}\n","  7% 3500/52629 [00:59<13:03, 62.72it/s][INFO|trainer.py:2640] 2022-08-02 19:28:26,672 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-3500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:28:26,677 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:28:26,740 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:28:26,744 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:28:26,748 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-3500/special_tokens_map.json\n","{'loss': 3.593, 'learning_rate': 2.7719888274525454e-05, 'epoch': 0.23}\n","  8% 4000/52629 [01:07<12:37, 64.19it/s][INFO|trainer.py:2640] 2022-08-02 19:28:34,692 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-4000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:28:34,697 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:28:34,750 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:28:34,754 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:28:34,758 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-4000/special_tokens_map.json\n","{'loss': 3.5155, 'learning_rate': 2.743487430884113e-05, 'epoch': 0.26}\n","  9% 4500/52629 [01:15<12:30, 64.14it/s][INFO|trainer.py:2640] 2022-08-02 19:28:42,913 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-4500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:28:42,919 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:28:42,974 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:28:42,978 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:28:42,982 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-4500/special_tokens_map.json\n","{'loss': 3.4417, 'learning_rate': 2.7149860343156814e-05, 'epoch': 0.29}\n"," 10% 5000/52629 [01:23<12:54, 61.48it/s][INFO|trainer.py:2640] 2022-08-02 19:28:51,184 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-5000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:28:51,189 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:28:51,248 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:28:51,252 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:28:51,256 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-5000/special_tokens_map.json\n","{'loss': 3.3628, 'learning_rate': 2.6864846377472498e-05, 'epoch': 0.31}\n"," 10% 5500/52629 [01:31<12:14, 64.15it/s][INFO|trainer.py:2640] 2022-08-02 19:28:59,197 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-5500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:28:59,202 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:28:59,255 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:28:59,259 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:28:59,263 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-5500/special_tokens_map.json\n","{'loss': 3.3508, 'learning_rate': 2.657983241178818e-05, 'epoch': 0.34}\n"," 11% 6000/52629 [01:39<12:10, 63.81it/s][INFO|trainer.py:2640] 2022-08-02 19:29:07,513 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-6000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:29:07,535 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:29:07,593 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:29:07,598 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:29:07,601 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-6000/special_tokens_map.json\n","{'loss': 3.3291, 'learning_rate': 2.6294818446103858e-05, 'epoch': 0.37}\n"," 12% 6500/52629 [01:48<12:24, 61.97it/s][INFO|trainer.py:2640] 2022-08-02 19:29:15,687 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-6500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:29:15,691 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:29:15,744 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:29:15,748 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:29:15,752 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-6500/special_tokens_map.json\n","{'loss': 3.2381, 'learning_rate': 2.600980448041954e-05, 'epoch': 0.4}\n"," 13% 7000/52629 [01:57<11:58, 63.54it/s][INFO|trainer.py:2640] 2022-08-02 19:29:24,651 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-7000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:29:24,656 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:29:24,711 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:29:24,715 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:29:24,719 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-7000/special_tokens_map.json\n","{'loss': 3.2735, 'learning_rate': 2.5724790514735224e-05, 'epoch': 0.43}\n"," 14% 7500/52629 [02:05<11:27, 65.67it/s][INFO|trainer.py:2640] 2022-08-02 19:29:32,772 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-7500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:29:32,782 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:29:32,837 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:29:32,841 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:29:32,860 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-7500/special_tokens_map.json\n","{'loss': 3.2627, 'learning_rate': 2.54397765490509e-05, 'epoch': 0.46}\n"," 15% 8000/52629 [02:13<13:09, 56.53it/s][INFO|trainer.py:2640] 2022-08-02 19:29:41,023 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-8000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:29:41,028 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:29:41,086 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:29:41,092 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:29:41,096 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-8000/special_tokens_map.json\n","{'loss': 3.146, 'learning_rate': 2.5154762583366584e-05, 'epoch': 0.48}\n"," 16% 8500/52629 [02:21<11:19, 64.95it/s][INFO|trainer.py:2640] 2022-08-02 19:29:49,154 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-8500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:29:49,159 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:29:49,224 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:29:49,232 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:29:49,236 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-8500/special_tokens_map.json\n","{'loss': 3.0925, 'learning_rate': 2.4869748617682268e-05, 'epoch': 0.51}\n"," 17% 9000/52629 [02:29<11:42, 62.15it/s][INFO|trainer.py:2640] 2022-08-02 19:29:57,487 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-9000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:29:57,492 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:29:57,546 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:29:57,550 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:29:57,573 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-9000/special_tokens_map.json\n","{'loss': 3.1043, 'learning_rate': 2.458473465199795e-05, 'epoch': 0.54}\n"," 18% 9500/52629 [02:37<12:13, 58.80it/s][INFO|trainer.py:2640] 2022-08-02 19:30:05,536 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-9500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:30:05,543 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:30:05,612 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:30:05,618 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:30:05,623 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-9500/special_tokens_map.json\n","{'loss': 3.0534, 'learning_rate': 2.4299720686313628e-05, 'epoch': 0.57}\n"," 19% 10000/52629 [02:46<11:03, 64.24it/s][INFO|trainer.py:2640] 2022-08-02 19:30:13,613 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-10000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:30:13,618 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:30:13,673 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:30:13,678 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:30:13,681 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-10000/special_tokens_map.json\n","{'loss': 3.122, 'learning_rate': 2.401470672062931e-05, 'epoch': 0.6}\n"," 20% 10500/52629 [02:54<11:33, 60.76it/s][INFO|trainer.py:2640] 2022-08-02 19:30:21,900 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-10500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:30:21,905 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:30:21,958 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:30:21,963 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:30:21,967 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-10500/special_tokens_map.json\n","{'loss': 3.0576, 'learning_rate': 2.3729692754944995e-05, 'epoch': 0.63}\n"," 21% 11000/52629 [03:02<11:06, 62.49it/s][INFO|trainer.py:2640] 2022-08-02 19:30:29,938 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-11000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:30:29,944 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:30:29,999 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:30:30,004 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:30:30,007 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-11000/special_tokens_map.json\n","{'loss': 3.0045, 'learning_rate': 2.344467878926067e-05, 'epoch': 0.66}\n"," 22% 11500/52629 [03:10<10:30, 65.18it/s][INFO|trainer.py:2640] 2022-08-02 19:30:38,063 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-11500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:30:38,068 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:30:38,122 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:30:38,126 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:30:38,130 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-11500/special_tokens_map.json\n","{'loss': 2.9903, 'learning_rate': 2.3159664823576355e-05, 'epoch': 0.68}\n"," 23% 12000/52629 [03:18<10:53, 62.20it/s][INFO|trainer.py:2640] 2022-08-02 19:30:46,293 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-12000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:30:46,300 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:30:46,363 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:30:46,367 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:30:46,371 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-12000/special_tokens_map.json\n","{'loss': 3.0002, 'learning_rate': 2.2874650857892038e-05, 'epoch': 0.71}\n"," 24% 12500/52629 [03:26<10:27, 63.95it/s][INFO|trainer.py:2640] 2022-08-02 19:30:54,308 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-12500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:30:54,313 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:30:54,365 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:30:54,369 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:30:54,372 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-12500/special_tokens_map.json\n","{'loss': 2.9681, 'learning_rate': 2.258963689220772e-05, 'epoch': 0.74}\n"," 25% 13000/52629 [03:35<10:22, 63.63it/s][INFO|trainer.py:2640] 2022-08-02 19:31:02,709 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-13000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:31:02,715 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:31:02,774 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:31:02,778 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:31:02,781 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-13000/special_tokens_map.json\n","{'loss': 2.9442, 'learning_rate': 2.2304622926523398e-05, 'epoch': 0.77}\n"," 26% 13500/52629 [03:44<11:18, 57.66it/s][INFO|trainer.py:2640] 2022-08-02 19:31:11,859 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-13500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:31:11,864 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:31:11,918 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:31:11,922 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:31:11,926 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-13500/special_tokens_map.json\n","{'loss': 2.9579, 'learning_rate': 2.201960896083908e-05, 'epoch': 0.8}\n"," 27% 14000/52629 [03:52<10:01, 64.22it/s][INFO|trainer.py:2640] 2022-08-02 19:31:19,835 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-14000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:31:19,841 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:31:19,894 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:31:19,899 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:31:19,902 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-14000/special_tokens_map.json\n","{'loss': 2.9277, 'learning_rate': 2.1734594995154765e-05, 'epoch': 0.83}\n"," 28% 14500/52629 [04:00<09:53, 64.21it/s][INFO|trainer.py:2640] 2022-08-02 19:31:28,079 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-14500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:31:28,097 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:31:28,152 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:31:28,156 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:31:28,160 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-14500/special_tokens_map.json\n","{'loss': 2.9364, 'learning_rate': 2.1449581029470445e-05, 'epoch': 0.86}\n"," 29% 15000/52629 [04:08<09:49, 63.86it/s][INFO|trainer.py:2640] 2022-08-02 19:31:36,492 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-15000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:31:36,497 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:31:36,549 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:31:36,553 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:31:36,557 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-15000/special_tokens_map.json\n","{'loss': 2.8828, 'learning_rate': 2.1164567063786125e-05, 'epoch': 0.88}\n"," 29% 15500/52629 [04:17<10:17, 60.12it/s][INFO|trainer.py:2640] 2022-08-02 19:31:44,601 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-15500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:31:44,605 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:31:44,658 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:31:44,662 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:31:44,676 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-15500/special_tokens_map.json\n","{'loss': 2.9341, 'learning_rate': 2.0879553098101808e-05, 'epoch': 0.91}\n"," 30% 16000/52629 [04:25<09:32, 63.99it/s][INFO|trainer.py:2640] 2022-08-02 19:31:52,791 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-16000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:31:52,797 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:31:52,859 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:31:52,863 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:31:52,866 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-16000/special_tokens_map.json\n","{'loss': 2.9137, 'learning_rate': 2.059453913241749e-05, 'epoch': 0.94}\n"," 31% 16500/52629 [04:33<09:50, 61.18it/s][INFO|trainer.py:2640] 2022-08-02 19:32:01,176 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-16500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:32:01,181 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:32:01,245 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:32:01,249 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:32:01,252 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-16500/special_tokens_map.json\n","{'loss': 2.8994, 'learning_rate': 2.0309525166733168e-05, 'epoch': 0.97}\n"," 32% 17000/52629 [04:41<09:16, 64.00it/s][INFO|trainer.py:2640] 2022-08-02 19:32:09,210 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-17000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:32:09,215 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-17000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:32:09,275 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-17000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:32:09,279 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-17000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:32:09,282 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-17000/special_tokens_map.json\n","{'loss': 2.8516, 'learning_rate': 2.002451120104885e-05, 'epoch': 1.0}\n"," 33% 17500/52629 [04:50<08:59, 65.16it/s][INFO|trainer.py:2640] 2022-08-02 19:32:17,587 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-17500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:32:17,594 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-17500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:32:17,648 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-17500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:32:17,653 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-17500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:32:17,656 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-17500/special_tokens_map.json\n","{'loss': 2.7599, 'learning_rate': 1.9739497235364535e-05, 'epoch': 1.03}\n"," 34% 18000/52629 [04:58<09:42, 59.43it/s][INFO|trainer.py:2640] 2022-08-02 19:32:25,739 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-18000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:32:25,744 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-18000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:32:25,810 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:32:25,815 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:32:25,818 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-18000/special_tokens_map.json\n","{'loss': 2.8004, 'learning_rate': 1.9454483269680215e-05, 'epoch': 1.05}\n"," 35% 18500/52629 [05:06<08:39, 65.72it/s][INFO|trainer.py:2640] 2022-08-02 19:32:33,859 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-18500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:32:33,865 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-18500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:32:33,931 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-18500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:32:33,936 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-18500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:32:33,946 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-18500/special_tokens_map.json\n","{'loss': 2.7549, 'learning_rate': 1.9169469303995895e-05, 'epoch': 1.08}\n"," 36% 19000/52629 [05:14<08:58, 62.50it/s][INFO|trainer.py:2640] 2022-08-02 19:32:42,224 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-19000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:32:42,230 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-19000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:32:42,306 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-19000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:32:42,310 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-19000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:32:42,314 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-19000/special_tokens_map.json\n","{'loss': 2.7771, 'learning_rate': 1.8884455338311578e-05, 'epoch': 1.11}\n"," 37% 19500/52629 [05:22<08:27, 65.24it/s][INFO|trainer.py:2640] 2022-08-02 19:32:50,223 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-19500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:32:50,228 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-19500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:32:50,318 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-19500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:32:50,325 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-19500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:32:50,330 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-19500/special_tokens_map.json\n","{'loss': 2.7675, 'learning_rate': 1.859944137262726e-05, 'epoch': 1.14}\n"," 38% 20000/52629 [05:31<11:31, 47.17it/s][INFO|trainer.py:2640] 2022-08-02 19:32:59,165 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-20000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:32:59,171 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-20000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:32:59,239 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-20000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:32:59,243 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-20000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:32:59,247 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-20000/special_tokens_map.json\n","{'loss': 2.6916, 'learning_rate': 1.8314427406942938e-05, 'epoch': 1.17}\n"," 39% 20500/52629 [05:40<08:28, 63.21it/s][INFO|trainer.py:2640] 2022-08-02 19:33:07,834 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-20500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:33:07,839 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-20500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:33:07,902 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-20500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:33:07,920 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-20500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:33:07,924 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-20500/special_tokens_map.json\n","{'loss': 2.7554, 'learning_rate': 1.802941344125862e-05, 'epoch': 1.2}\n"," 40% 21000/52629 [05:48<08:22, 62.90it/s][INFO|trainer.py:2640] 2022-08-02 19:33:16,053 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-21000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:33:16,058 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-21000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:33:16,112 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:33:16,117 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:33:16,120 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-21000/special_tokens_map.json\n","{'loss': 2.7621, 'learning_rate': 1.7744399475574305e-05, 'epoch': 1.23}\n"," 41% 21500/52629 [05:56<07:55, 65.49it/s][INFO|trainer.py:2640] 2022-08-02 19:33:24,279 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-21500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:33:24,284 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-21500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:33:24,345 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-21500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:33:24,349 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-21500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:33:24,353 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-21500/special_tokens_map.json\n","{'loss': 2.7171, 'learning_rate': 1.7459385509889985e-05, 'epoch': 1.25}\n"," 42% 22000/52629 [06:05<08:02, 63.43it/s][INFO|trainer.py:2640] 2022-08-02 19:33:32,741 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-22000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:33:32,746 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-22000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:33:32,806 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-22000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:33:32,826 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-22000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:33:32,829 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-22000/special_tokens_map.json\n","{'loss': 2.7549, 'learning_rate': 1.7174371544205665e-05, 'epoch': 1.28}\n"," 43% 22500/52629 [06:13<09:20, 53.76it/s][INFO|trainer.py:2640] 2022-08-02 19:33:40,956 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-22500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:33:40,961 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:33:41,017 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-22500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:33:41,021 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-22500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:33:41,025 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-22500/special_tokens_map.json\n","{'loss': 2.7533, 'learning_rate': 1.688935757852135e-05, 'epoch': 1.31}\n"," 44% 23000/52629 [06:21<07:43, 63.92it/s][INFO|trainer.py:2640] 2022-08-02 19:33:49,077 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-23000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:33:49,082 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-23000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:33:49,149 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-23000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:33:49,153 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-23000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:33:49,157 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-23000/special_tokens_map.json\n","{'loss': 2.7607, 'learning_rate': 1.6604343612837032e-05, 'epoch': 1.34}\n"," 45% 23500/52629 [06:29<07:47, 62.29it/s][INFO|trainer.py:2640] 2022-08-02 19:33:57,432 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-23500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:33:57,454 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-23500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:33:57,532 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-23500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:33:57,551 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-23500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:33:57,556 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-23500/special_tokens_map.json\n","{'loss': 2.7582, 'learning_rate': 1.631932964715271e-05, 'epoch': 1.37}\n"," 46% 24000/52629 [06:38<07:59, 59.70it/s][INFO|trainer.py:2640] 2022-08-02 19:34:05,581 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-24000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:34:05,586 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-24000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:34:05,643 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:34:05,647 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:34:05,651 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-24000/special_tokens_map.json\n","{'loss': 2.7138, 'learning_rate': 1.6034315681468392e-05, 'epoch': 1.4}\n"," 47% 24500/52629 [06:46<07:08, 65.72it/s][INFO|trainer.py:2640] 2022-08-02 19:34:13,649 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-24500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:34:13,654 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-24500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:34:13,712 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-24500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:34:13,716 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-24500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:34:13,720 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-24500/special_tokens_map.json\n","{'loss': 2.7392, 'learning_rate': 1.5749301715784075e-05, 'epoch': 1.43}\n"," 48% 25000/52629 [06:54<07:12, 63.90it/s][INFO|trainer.py:2640] 2022-08-02 19:34:21,912 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-25000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:34:21,917 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-25000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:34:21,972 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-25000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:34:21,976 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-25000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:34:22,001 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-25000/special_tokens_map.json\n","{'loss': 2.6482, 'learning_rate': 1.5464287750099755e-05, 'epoch': 1.45}\n"," 48% 25500/52629 [07:02<07:02, 64.18it/s][INFO|trainer.py:2640] 2022-08-02 19:34:29,893 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-25500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:34:29,898 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-25500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:34:29,960 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-25500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:34:29,964 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-25500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:34:29,967 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-25500/special_tokens_map.json\n","{'loss': 2.7164, 'learning_rate': 1.5179273784415437e-05, 'epoch': 1.48}\n"," 49% 26000/52629 [07:10<06:56, 63.92it/s][INFO|trainer.py:2640] 2022-08-02 19:34:38,191 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-26000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:34:38,196 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-26000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:34:38,256 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-26000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:34:38,260 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-26000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:34:38,264 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-26000/special_tokens_map.json\n","{'loss': 2.7996, 'learning_rate': 1.4894259818731119e-05, 'epoch': 1.51}\n"," 50% 26500/52629 [07:18<06:59, 62.30it/s][INFO|trainer.py:2640] 2022-08-02 19:34:46,547 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-26500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:34:46,552 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-26500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:34:46,607 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-26500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:34:46,612 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-26500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:34:46,615 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-26500/special_tokens_map.json\n","{'loss': 2.7637, 'learning_rate': 1.4609245853046799e-05, 'epoch': 1.54}\n"," 51% 27000/52629 [07:28<06:34, 64.89it/s][INFO|trainer.py:2640] 2022-08-02 19:34:55,691 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-27000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:34:55,703 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-27000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:34:55,781 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-27000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:34:55,787 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-27000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:34:55,793 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-27000/special_tokens_map.json\n","{'loss': 2.7042, 'learning_rate': 1.4324231887362482e-05, 'epoch': 1.57}\n"," 52% 27500/52629 [07:36<06:42, 62.38it/s][INFO|trainer.py:2640] 2022-08-02 19:35:03,922 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-27500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:35:03,927 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-27500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:35:03,981 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-27500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:35:03,986 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-27500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:35:03,989 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-27500/special_tokens_map.json\n","{'loss': 2.6762, 'learning_rate': 1.4039217921678162e-05, 'epoch': 1.6}\n"," 53% 28000/52629 [07:44<06:50, 59.99it/s][INFO|trainer.py:2640] 2022-08-02 19:35:12,311 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-28000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:35:12,316 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-28000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:35:12,372 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-28000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:35:12,376 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-28000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:35:12,379 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-28000/special_tokens_map.json\n","{'loss': 2.7382, 'learning_rate': 1.3754203955993845e-05, 'epoch': 1.62}\n"," 54% 28500/52629 [07:52<06:30, 61.74it/s][INFO|trainer.py:2640] 2022-08-02 19:35:20,390 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-28500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:35:20,397 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-28500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:35:20,470 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-28500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:35:20,476 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-28500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:35:20,480 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-28500/special_tokens_map.json\n","{'loss': 2.7503, 'learning_rate': 1.3469189990309525e-05, 'epoch': 1.65}\n"," 55% 29000/52629 [08:01<06:05, 64.71it/s][INFO|trainer.py:2640] 2022-08-02 19:35:28,580 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-29000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:35:28,585 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-29000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:35:28,639 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-29000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:35:28,643 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-29000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:35:28,647 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-29000/special_tokens_map.json\n","{'loss': 2.7201, 'learning_rate': 1.3184176024625207e-05, 'epoch': 1.68}\n"," 56% 29500/52629 [08:09<06:31, 59.14it/s][INFO|trainer.py:2640] 2022-08-02 19:35:37,073 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-29500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:35:37,078 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-29500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:35:37,134 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-29500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:35:37,138 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-29500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:35:37,142 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-29500/special_tokens_map.json\n","{'loss': 2.6753, 'learning_rate': 1.2899162058940889e-05, 'epoch': 1.71}\n"," 57% 30000/52629 [08:17<06:45, 55.82it/s][INFO|trainer.py:2640] 2022-08-02 19:35:45,288 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-30000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:35:45,292 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-30000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:35:45,348 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-30000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:35:45,352 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-30000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:35:45,355 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-30000/special_tokens_map.json\n","{'loss': 2.7339, 'learning_rate': 1.2614148093256569e-05, 'epoch': 1.74}\n"," 58% 30500/52629 [08:25<05:45, 64.08it/s][INFO|trainer.py:2640] 2022-08-02 19:35:53,438 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-30500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:35:53,443 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-30500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:35:53,501 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-30500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:35:53,505 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-30500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:35:53,508 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-30500/special_tokens_map.json\n","{'loss': 2.7222, 'learning_rate': 1.2329134127572252e-05, 'epoch': 1.77}\n"," 59% 31000/52629 [08:34<05:43, 62.92it/s][INFO|trainer.py:2640] 2022-08-02 19:36:01,820 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-31000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:36:01,825 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-31000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:36:01,887 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-31000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:36:01,891 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-31000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:36:01,894 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-31000/special_tokens_map.json\n","{'loss': 2.6654, 'learning_rate': 1.2044120161887932e-05, 'epoch': 1.8}\n"," 60% 31500/52629 [08:42<05:35, 62.98it/s][INFO|trainer.py:2640] 2022-08-02 19:36:09,936 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-31500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:36:09,941 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-31500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:36:10,015 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-31500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:36:10,019 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-31500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:36:10,023 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-31500/special_tokens_map.json\n","{'loss': 2.7559, 'learning_rate': 1.1759106196203616e-05, 'epoch': 1.82}\n"," 61% 32000/52629 [08:50<05:18, 64.86it/s][INFO|trainer.py:2640] 2022-08-02 19:36:18,170 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-32000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:36:18,191 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-32000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:36:18,250 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-32000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:36:18,254 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-32000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:36:18,258 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-32000/special_tokens_map.json\n","{'loss': 2.6557, 'learning_rate': 1.1474092230519296e-05, 'epoch': 1.85}\n"," 62% 32500/52629 [08:58<05:22, 62.44it/s][INFO|trainer.py:2640] 2022-08-02 19:36:26,510 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-32500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:36:26,515 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-32500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:36:26,587 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-32500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:36:26,594 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-32500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:36:26,597 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-32500/special_tokens_map.json\n","{'loss': 2.6873, 'learning_rate': 1.1189078264834977e-05, 'epoch': 1.88}\n"," 63% 33000/52629 [09:07<05:03, 64.68it/s][INFO|trainer.py:2640] 2022-08-02 19:36:34,662 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-33000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:36:34,670 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-33000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:36:34,748 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-33000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:36:34,753 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-33000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:36:34,756 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-33000/special_tokens_map.json\n","{'loss': 2.7063, 'learning_rate': 1.0904064299150659e-05, 'epoch': 1.91}\n"," 64% 33500/52629 [09:16<04:59, 63.91it/s][INFO|trainer.py:2640] 2022-08-02 19:36:44,197 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-33500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:36:44,203 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-33500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:36:44,263 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-33500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:36:44,267 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-33500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:36:44,270 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-33500/special_tokens_map.json\n","{'loss': 2.6581, 'learning_rate': 1.0619050333466339e-05, 'epoch': 1.94}\n"," 65% 34000/52629 [09:24<04:56, 62.79it/s][INFO|trainer.py:2640] 2022-08-02 19:36:52,456 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-34000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:36:52,461 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-34000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:36:52,534 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-34000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:36:52,538 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-34000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:36:52,541 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-34000/special_tokens_map.json\n","{'loss': 2.703, 'learning_rate': 1.0334036367782022e-05, 'epoch': 1.97}\n"," 66% 34500/52629 [09:33<05:25, 55.67it/s][INFO|trainer.py:2640] 2022-08-02 19:37:00,645 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-34500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:37:00,650 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-34500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:37:00,725 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-34500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:37:00,730 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-34500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:37:00,733 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-34500/special_tokens_map.json\n","{'loss': 2.625, 'learning_rate': 1.0049022402097702e-05, 'epoch': 2.0}\n"," 67% 35000/52629 [09:41<04:39, 62.97it/s][INFO|trainer.py:2640] 2022-08-02 19:37:08,852 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-35000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:37:09,005 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-35000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:37:09,065 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-35000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:37:09,072 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-35000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:37:09,077 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-35000/special_tokens_map.json\n","{'loss': 2.6155, 'learning_rate': 9.764008436413386e-06, 'epoch': 2.02}\n"," 67% 35500/52629 [09:49<04:27, 64.15it/s][INFO|trainer.py:2640] 2022-08-02 19:37:17,467 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-35500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:37:17,472 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-35500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:37:17,535 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-35500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:37:17,539 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-35500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:37:17,542 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-35500/special_tokens_map.json\n","{'loss': 2.5546, 'learning_rate': 9.478994470729066e-06, 'epoch': 2.05}\n"," 68% 36000/52629 [09:58<05:05, 54.44it/s][INFO|trainer.py:2640] 2022-08-02 19:37:25,604 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-36000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:37:25,609 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-36000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:37:25,668 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-36000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:37:25,672 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-36000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:37:25,676 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-36000/special_tokens_map.json\n","{'loss': 2.6148, 'learning_rate': 9.193980505044747e-06, 'epoch': 2.08}\n"," 69% 36500/52629 [10:06<04:09, 64.75it/s][INFO|trainer.py:2640] 2022-08-02 19:37:33,760 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-36500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:37:33,767 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-36500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:37:33,825 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-36500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:37:33,847 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-36500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:37:33,850 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-36500/special_tokens_map.json\n","{'loss': 2.5755, 'learning_rate': 8.908966539360429e-06, 'epoch': 2.11}\n"," 70% 37000/52629 [10:14<04:21, 59.78it/s][INFO|trainer.py:2640] 2022-08-02 19:37:42,211 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-37000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:37:42,217 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-37000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:37:42,277 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-37000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:37:42,281 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-37000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:37:42,284 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-37000/special_tokens_map.json\n","{'loss': 2.5831, 'learning_rate': 8.623952573676109e-06, 'epoch': 2.14}\n"," 71% 37500/52629 [10:22<03:58, 63.34it/s][INFO|trainer.py:2640] 2022-08-02 19:37:50,332 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-37500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:37:50,337 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-37500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:37:50,416 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-37500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:37:50,424 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-37500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:37:50,430 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-37500/special_tokens_map.json\n","{'loss': 2.6104, 'learning_rate': 8.338938607991792e-06, 'epoch': 2.17}\n"," 72% 38000/52629 [10:31<03:46, 64.65it/s][INFO|trainer.py:2640] 2022-08-02 19:37:58,569 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-38000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:37:58,574 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-38000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:37:58,644 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-38000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:37:58,649 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-38000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:37:58,670 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-38000/special_tokens_map.json\n","{'loss': 2.5912, 'learning_rate': 8.053924642307472e-06, 'epoch': 2.19}\n"," 73% 38500/52629 [10:39<03:48, 61.88it/s][INFO|trainer.py:2640] 2022-08-02 19:38:06,957 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-38500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:38:06,963 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-38500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:38:07,018 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-38500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:38:07,022 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-38500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:38:07,026 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-38500/special_tokens_map.json\n","{'loss': 2.5808, 'learning_rate': 7.768910676623156e-06, 'epoch': 2.22}\n"," 74% 39000/52629 [10:47<03:49, 59.30it/s][INFO|trainer.py:2640] 2022-08-02 19:38:15,205 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-39000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:38:15,210 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-39000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:38:15,273 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-39000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:38:15,277 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-39000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:38:15,280 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-39000/special_tokens_map.json\n","{'loss': 2.5927, 'learning_rate': 7.483896710938836e-06, 'epoch': 2.25}\n"," 75% 39500/52629 [10:55<03:25, 63.75it/s][INFO|trainer.py:2640] 2022-08-02 19:38:23,315 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-39500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:38:23,321 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-39500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:38:23,393 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-39500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:38:23,397 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-39500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:38:23,401 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-39500/special_tokens_map.json\n","{'loss': 2.5863, 'learning_rate': 7.1988827452545175e-06, 'epoch': 2.28}\n"," 76% 40000/52629 [11:05<03:18, 63.72it/s][INFO|trainer.py:2640] 2022-08-02 19:38:32,828 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-40000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:38:32,833 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-40000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:38:32,897 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-40000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:38:32,901 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-40000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:38:32,904 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-40000/special_tokens_map.json\n","{'loss': 2.589, 'learning_rate': 6.913868779570199e-06, 'epoch': 2.31}\n"," 77% 40500/52629 [11:13<03:43, 54.34it/s][INFO|trainer.py:2640] 2022-08-02 19:38:41,174 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-40500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:38:41,179 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-40500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:38:41,240 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-40500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:38:41,245 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-40500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:38:41,248 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-40500/special_tokens_map.json\n","{'loss': 2.6383, 'learning_rate': 6.628854813885881e-06, 'epoch': 2.34}\n"," 78% 41000/52629 [11:21<02:59, 64.68it/s][INFO|trainer.py:2640] 2022-08-02 19:38:49,348 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-41000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:38:49,353 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-41000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:38:49,416 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-41000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:38:49,420 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-41000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:38:49,424 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-41000/special_tokens_map.json\n","{'loss': 2.5883, 'learning_rate': 6.343840848201562e-06, 'epoch': 2.37}\n"," 79% 41500/52629 [11:30<02:59, 61.90it/s][INFO|trainer.py:2640] 2022-08-02 19:38:57,863 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-41500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:38:57,868 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-41500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:38:57,952 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-41500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:38:57,961 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-41500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:38:57,965 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-41500/special_tokens_map.json\n","{'loss': 2.6081, 'learning_rate': 6.0588268825172434e-06, 'epoch': 2.39}\n"," 80% 42000/52629 [11:38<03:06, 56.85it/s][INFO|trainer.py:2640] 2022-08-02 19:39:06,234 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-42000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:39:06,239 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-42000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:39:06,297 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-42000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:39:06,302 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-42000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:39:06,305 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-42000/special_tokens_map.json\n","{'loss': 2.5842, 'learning_rate': 5.773812916832925e-06, 'epoch': 2.42}\n"," 81% 42500/52629 [11:46<02:39, 63.67it/s][INFO|trainer.py:2640] 2022-08-02 19:39:14,457 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-42500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:39:14,462 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-42500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:39:14,520 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-42500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:39:14,524 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-42500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:39:14,528 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-42500/special_tokens_map.json\n","{'loss': 2.5788, 'learning_rate': 5.488798951148607e-06, 'epoch': 2.45}\n"," 82% 43000/52629 [11:55<02:35, 62.00it/s][INFO|trainer.py:2640] 2022-08-02 19:39:22,932 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-43000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:39:22,937 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-43000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:39:23,018 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-43000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:39:23,022 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-43000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:39:23,025 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-43000/special_tokens_map.json\n","{'loss': 2.5318, 'learning_rate': 5.203784985464288e-06, 'epoch': 2.48}\n"," 83% 43500/52629 [12:03<02:43, 55.78it/s][INFO|trainer.py:2640] 2022-08-02 19:39:31,233 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-43500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:39:31,238 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-43500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:39:31,308 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-43500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:39:31,312 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-43500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:39:31,315 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-43500/special_tokens_map.json\n","{'loss': 2.6007, 'learning_rate': 4.918771019779969e-06, 'epoch': 2.51}\n"," 84% 44000/52629 [12:11<02:15, 63.57it/s][INFO|trainer.py:2640] 2022-08-02 19:39:39,425 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-44000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:39:39,431 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-44000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:39:39,500 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-44000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:39:39,505 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-44000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:39:39,508 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-44000/special_tokens_map.json\n","{'loss': 2.5745, 'learning_rate': 4.633757054095651e-06, 'epoch': 2.54}\n"," 85% 44500/52629 [12:20<02:09, 62.85it/s][INFO|trainer.py:2640] 2022-08-02 19:39:47,895 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-44500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:39:47,900 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-44500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:39:47,961 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-44500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:39:47,965 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-44500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:39:47,969 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-44500/special_tokens_map.json\n","{'loss': 2.5289, 'learning_rate': 4.348743088411332e-06, 'epoch': 2.57}\n"," 86% 45000/52629 [12:28<02:26, 51.90it/s][INFO|trainer.py:2640] 2022-08-02 19:39:56,206 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-45000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:39:56,217 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-45000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:39:56,281 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-45000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:39:56,285 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-45000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:39:56,288 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-45000/special_tokens_map.json\n","{'loss': 2.5818, 'learning_rate': 4.063729122727014e-06, 'epoch': 2.59}\n"," 86% 45500/52629 [12:36<01:51, 63.94it/s][INFO|trainer.py:2640] 2022-08-02 19:40:04,412 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-45500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:40:04,417 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-45500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:40:04,480 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-45500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:40:04,484 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-45500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:40:04,488 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-45500/special_tokens_map.json\n","{'loss': 2.5497, 'learning_rate': 3.778715157042695e-06, 'epoch': 2.62}\n"," 87% 46000/52629 [12:45<01:44, 63.43it/s][INFO|trainer.py:2640] 2022-08-02 19:40:12,907 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-46000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:40:12,912 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-46000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:40:12,974 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-46000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:40:12,984 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-46000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:40:12,987 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-46000/special_tokens_map.json\n","{'loss': 2.5743, 'learning_rate': 3.4937011913583766e-06, 'epoch': 2.65}\n"," 88% 46500/52629 [12:54<01:47, 57.22it/s][INFO|trainer.py:2640] 2022-08-02 19:40:22,170 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-46500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:40:22,175 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-46500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:40:22,247 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-46500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:40:22,251 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-46500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:40:22,255 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-46500/special_tokens_map.json\n","{'loss': 2.5017, 'learning_rate': 3.2086872256740582e-06, 'epoch': 2.68}\n"," 89% 47000/52629 [13:02<01:26, 64.97it/s][INFO|trainer.py:2640] 2022-08-02 19:40:30,276 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-47000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:40:30,281 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-47000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:40:30,341 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-47000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:40:30,345 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-47000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:40:30,349 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-47000/special_tokens_map.json\n","{'loss': 2.5483, 'learning_rate': 2.92367325998974e-06, 'epoch': 2.71}\n"," 90% 47500/52629 [13:11<01:28, 58.24it/s][INFO|trainer.py:2640] 2022-08-02 19:40:38,754 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-47500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:40:38,759 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-47500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:40:38,818 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-47500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:40:38,822 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-47500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:40:38,826 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-47500/special_tokens_map.json\n","{'loss': 2.6047, 'learning_rate': 2.6386592943054208e-06, 'epoch': 2.74}\n"," 91% 48000/52629 [13:19<01:12, 63.83it/s][INFO|trainer.py:2640] 2022-08-02 19:40:47,057 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-48000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:40:47,062 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-48000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:40:47,125 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-48000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:40:47,129 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-48000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:40:47,133 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-48000/special_tokens_map.json\n","{'loss': 2.5553, 'learning_rate': 2.3536453286211025e-06, 'epoch': 2.76}\n"," 92% 48500/52629 [13:27<01:04, 63.71it/s][INFO|trainer.py:2640] 2022-08-02 19:40:55,213 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-48500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:40:55,218 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-48500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:40:55,278 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-48500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:40:55,282 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-48500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:40:55,285 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-48500/special_tokens_map.json\n","{'loss': 2.6252, 'learning_rate': 2.068631362936784e-06, 'epoch': 2.79}\n"," 93% 49000/52629 [13:36<00:58, 61.84it/s][INFO|trainer.py:2640] 2022-08-02 19:41:03,717 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-49000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:41:03,722 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-49000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:41:03,783 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-49000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:41:03,787 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-49000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:41:03,791 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-49000/special_tokens_map.json\n","{'loss': 2.5519, 'learning_rate': 1.7836173972524654e-06, 'epoch': 2.82}\n"," 94% 49500/52629 [13:44<00:50, 62.15it/s][INFO|trainer.py:2640] 2022-08-02 19:41:11,931 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-49500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:41:11,936 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-49500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:41:12,015 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-49500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:41:12,020 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-49500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:41:12,024 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-49500/special_tokens_map.json\n","{'loss': 2.5817, 'learning_rate': 1.498603431568147e-06, 'epoch': 2.85}\n"," 95% 50000/52629 [13:52<00:41, 63.04it/s][INFO|trainer.py:2640] 2022-08-02 19:41:20,107 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-50000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:41:20,111 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-50000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:41:20,174 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-50000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:41:20,178 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-50000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:41:20,181 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-50000/special_tokens_map.json\n","{'loss': 2.5457, 'learning_rate': 1.2135894658838284e-06, 'epoch': 2.88}\n"," 96% 50500/52629 [14:01<00:33, 63.63it/s][INFO|trainer.py:2640] 2022-08-02 19:41:28,565 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-50500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:41:28,571 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-50500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:41:28,630 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-50500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:41:28,635 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-50500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:41:28,640 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-50500/special_tokens_map.json\n","{'loss': 2.5664, 'learning_rate': 9.285755001995098e-07, 'epoch': 2.91}\n"," 97% 51000/52629 [14:09<00:27, 60.02it/s][INFO|trainer.py:2640] 2022-08-02 19:41:36,814 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-51000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:41:36,831 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-51000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:41:36,902 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-51000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:41:36,906 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-51000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:41:36,910 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-51000/special_tokens_map.json\n","{'loss': 2.5883, 'learning_rate': 6.435615345151913e-07, 'epoch': 2.94}\n"," 98% 51500/52629 [14:17<00:19, 56.74it/s][INFO|trainer.py:2640] 2022-08-02 19:41:45,210 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-51500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:41:45,215 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-51500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:41:45,270 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-51500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:41:45,274 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-51500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:41:45,278 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-51500/special_tokens_map.json\n","{'loss': 2.5447, 'learning_rate': 3.5854756883087273e-07, 'epoch': 2.96}\n"," 99% 52000/52629 [14:25<00:09, 63.99it/s][INFO|trainer.py:2640] 2022-08-02 19:41:53,399 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-52000\n","[INFO|configuration_utils.py:451] 2022-08-02 19:41:53,404 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-52000/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:41:53,464 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-52000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:41:53,469 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-52000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:41:53,472 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-52000/special_tokens_map.json\n","{'loss': 2.5813, 'learning_rate': 7.353360314655419e-08, 'epoch': 2.99}\n","100% 52500/52629 [14:34<00:02, 59.79it/s][INFO|trainer.py:2640] 2022-08-02 19:42:01,857 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-52500\n","[INFO|configuration_utils.py:451] 2022-08-02 19:42:01,865 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-52500/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:42:01,923 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-52500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:42:01,927 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-52500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:42:01,931 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/checkpoint-52500/special_tokens_map.json\n","100% 52626/52629 [14:37<00:00, 43.14it/s][INFO|trainer.py:1850] 2022-08-02 19:42:04,853 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 877.6242, 'train_samples_per_second': 299.834, 'train_steps_per_second': 59.968, 'train_loss': 2.87008975942873, 'epoch': 3.0}\n","100% 52629/52629 [14:37<00:00, 59.99it/s]\n","[INFO|trainer.py:2640] 2022-08-02 19:42:04,860 >> Saving model checkpoint to ./montura/MyDrive/TFG/minibert/mini_model\n","[INFO|configuration_utils.py:451] 2022-08-02 19:42:04,865 >> Configuration saved in ./montura/MyDrive/TFG/minibert/mini_model/config.json\n","[INFO|modeling_utils.py:1566] 2022-08-02 19:42:04,983 >> Model weights saved in ./montura/MyDrive/TFG/minibert/mini_model/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2145] 2022-08-02 19:42:04,987 >> tokenizer config file saved in ./montura/MyDrive/TFG/minibert/mini_model/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2152] 2022-08-02 19:42:04,992 >> Special tokens file saved in ./montura/MyDrive/TFG/minibert/mini_model/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     2.8701\n","  train_runtime            = 0:14:37.62\n","  train_samples            =      87714\n","  train_samples_per_second =    299.834\n","  train_steps_per_second   =     59.968\n","08/02/2022 19:42:05 - INFO - mylogger - *** Evaluate ***\n","[INFO|trainer.py:723] 2022-08-02 19:42:05,268 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2891] 2022-08-02 19:42:05,279 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2893] 2022-08-02 19:42:05,279 >>   Num examples = 10626\n","[INFO|trainer.py:2896] 2022-08-02 19:42:05,279 >>   Batch size = 8\n"," 99% 1321/1329 [00:13<00:00, 104.28it/s]08/02/2022 19:42:29 - INFO - utils_qa - Post-processing 10570 example predictions split into 10626 features.\n","\n","  0% 0/10570 [00:00<?, ?it/s]\u001b[A\n","  0% 34/10570 [00:00<00:31, 332.48it/s]\u001b[A\n","  1% 77/10570 [00:00<00:27, 386.40it/s]\u001b[A\n","  1% 117/10570 [00:00<00:26, 390.14it/s]\u001b[A\n","  2% 161/10570 [00:00<00:25, 406.67it/s]\u001b[A\n","  2% 203/10570 [00:00<00:25, 409.60it/s]\u001b[A\n","  2% 244/10570 [00:00<00:27, 374.16it/s]\u001b[A\n","  3% 282/10570 [00:00<00:29, 347.21it/s]\u001b[A\n","  3% 319/10570 [00:00<00:29, 351.47it/s]\u001b[A\n","  3% 360/10570 [00:00<00:27, 367.80it/s]\u001b[A\n","  4% 398/10570 [00:01<00:27, 369.59it/s]\u001b[A\n","  4% 438/10570 [00:01<00:26, 377.74it/s]\u001b[A\n","  5% 478/10570 [00:01<00:26, 382.33it/s]\u001b[A\n","  5% 518/10570 [00:01<00:26, 386.05it/s]\u001b[A\n","  5% 559/10570 [00:01<00:25, 392.47it/s]\u001b[A\n","  6% 599/10570 [00:01<00:25, 394.69it/s]\u001b[A\n","  6% 639/10570 [00:01<00:26, 378.98it/s]\u001b[A\n","  6% 678/10570 [00:01<00:26, 374.54it/s]\u001b[A\n","  7% 717/10570 [00:01<00:26, 377.01it/s]\u001b[A\n","  7% 755/10570 [00:02<00:26, 370.42it/s]\u001b[A\n","  8% 793/10570 [00:02<00:26, 365.47it/s]\u001b[A\n","  8% 830/10570 [00:02<00:26, 361.14it/s]\u001b[A\n","  8% 867/10570 [00:02<00:27, 355.62it/s]\u001b[A\n","  9% 906/10570 [00:02<00:26, 365.33it/s]\u001b[A\n","  9% 944/10570 [00:02<00:26, 369.06it/s]\u001b[A\n","  9% 981/10570 [00:02<00:27, 354.98it/s]\u001b[A\n"," 10% 1019/10570 [00:02<00:26, 360.92it/s]\u001b[A\n"," 10% 1056/10570 [00:02<00:26, 358.98it/s]\u001b[A\n"," 10% 1092/10570 [00:02<00:26, 356.72it/s]\u001b[A\n"," 11% 1131/10570 [00:03<00:25, 366.02it/s]\u001b[A\n"," 11% 1173/10570 [00:03<00:24, 380.82it/s]\u001b[A\n"," 11% 1212/10570 [00:03<00:24, 376.48it/s]\u001b[A\n"," 12% 1252/10570 [00:03<00:24, 382.86it/s]\u001b[A\n"," 12% 1293/10570 [00:03<00:23, 389.32it/s]\u001b[A\n"," 13% 1332/10570 [00:03<00:24, 381.81it/s]\u001b[A\n"," 13% 1371/10570 [00:03<00:24, 379.85it/s]\u001b[A\n"," 13% 1410/10570 [00:03<00:24, 379.47it/s]\u001b[A\n"," 14% 1448/10570 [00:03<00:24, 366.89it/s]\u001b[A\n"," 14% 1487/10570 [00:03<00:24, 371.95it/s]\u001b[A\n"," 14% 1529/10570 [00:04<00:23, 385.83it/s]\u001b[A\n"," 15% 1568/10570 [00:04<00:23, 376.14it/s]\u001b[A\n"," 15% 1611/10570 [00:04<00:22, 389.83it/s]\u001b[A\n"," 16% 1651/10570 [00:04<00:22, 388.78it/s]\u001b[A\n"," 16% 1690/10570 [00:04<00:22, 388.38it/s]\u001b[A\n"," 16% 1731/10570 [00:04<00:22, 394.16it/s]\u001b[A\n"," 17% 1771/10570 [00:04<00:22, 389.66it/s]\u001b[A\n"," 17% 1811/10570 [00:04<00:23, 378.96it/s]\u001b[A\n"," 18% 1851/10570 [00:04<00:22, 383.57it/s]\u001b[A\n"," 18% 1890/10570 [00:05<00:22, 383.73it/s]\u001b[A\n"," 18% 1932/10570 [00:05<00:21, 393.79it/s]\u001b[A\n"," 19% 1974/10570 [00:05<00:21, 401.09it/s]\u001b[A\n"," 19% 2015/10570 [00:05<00:22, 385.99it/s]\u001b[A\n"," 19% 2056/10570 [00:05<00:21, 391.64it/s]\u001b[A\n"," 20% 2099/10570 [00:05<00:21, 401.60it/s]\u001b[A\n"," 20% 2140/10570 [00:05<00:22, 368.24it/s]\u001b[A\n"," 21% 2179/10570 [00:05<00:22, 372.57it/s]\u001b[A\n"," 21% 2218/10570 [00:05<00:22, 376.47it/s]\u001b[A\n"," 21% 2256/10570 [00:05<00:22, 365.69it/s]\u001b[A\n"," 22% 2297/10570 [00:06<00:21, 377.92it/s]\u001b[A\n"," 22% 2338/10570 [00:06<00:21, 385.00it/s]\u001b[A\n"," 22% 2377/10570 [00:06<00:21, 373.49it/s]\u001b[A\n"," 23% 2416/10570 [00:06<00:21, 376.03it/s]\u001b[A\n"," 23% 2454/10570 [00:06<00:21, 375.31it/s]\u001b[A\n"," 24% 2492/10570 [00:06<00:22, 366.71it/s]\u001b[A\n"," 24% 2529/10570 [00:06<00:22, 365.40it/s]\u001b[A\n"," 24% 2566/10570 [00:06<00:22, 351.00it/s]\u001b[A\n"," 25% 2602/10570 [00:06<00:22, 350.11it/s]\u001b[A\n"," 25% 2643/10570 [00:07<00:21, 367.00it/s]\u001b[A\n"," 25% 2680/10570 [00:07<00:21, 363.81it/s]\u001b[A\n"," 26% 2719/10570 [00:07<00:21, 370.74it/s]\u001b[A\n"," 26% 2762/10570 [00:07<00:20, 386.41it/s]\u001b[A\n"," 26% 2801/10570 [00:07<00:20, 384.28it/s]\u001b[A\n"," 27% 2841/10570 [00:07<00:19, 386.52it/s]\u001b[A\n"," 27% 2881/10570 [00:07<00:19, 387.43it/s]\u001b[A\n"," 28% 2920/10570 [00:07<00:20, 373.79it/s]\u001b[A\n"," 28% 2958/10570 [00:07<00:20, 370.43it/s]\u001b[A\n"," 28% 2997/10570 [00:07<00:20, 373.69it/s]\u001b[A\n","100% 1329/1329 [00:32<00:00, 104.28it/s]\n"," 29% 3073/10570 [00:08<00:20, 368.90it/s]\u001b[A\n"," 29% 3112/10570 [00:08<00:20, 371.74it/s]\u001b[A\n"," 30% 3150/10570 [00:08<00:20, 362.20it/s]\u001b[A\n"," 30% 3188/10570 [00:08<00:20, 367.19it/s]\u001b[A\n"," 31% 3227/10570 [00:08<00:19, 373.20it/s]\u001b[A\n"," 31% 3265/10570 [00:08<00:20, 361.51it/s]\u001b[A\n"," 31% 3305/10570 [00:08<00:19, 370.33it/s]\u001b[A\n"," 32% 3343/10570 [00:08<00:19, 368.74it/s]\u001b[A\n"," 32% 3380/10570 [00:09<00:19, 362.26it/s]\u001b[A\n"," 32% 3419/10570 [00:09<00:19, 369.87it/s]\u001b[A\n"," 33% 3458/10570 [00:09<00:19, 373.73it/s]\u001b[A\n"," 33% 3496/10570 [00:09<00:19, 362.47it/s]\u001b[A\n"," 33% 3536/10570 [00:09<00:18, 373.00it/s]\u001b[A\n"," 34% 3574/10570 [00:09<00:19, 362.97it/s]\u001b[A\n"," 34% 3613/10570 [00:09<00:18, 368.43it/s]\u001b[A\n"," 35% 3653/10570 [00:09<00:18, 375.39it/s]\u001b[A\n"," 35% 3691/10570 [00:09<00:18, 364.56it/s]\u001b[A\n"," 35% 3728/10570 [00:09<00:18, 361.83it/s]\u001b[A\n"," 36% 3767/10570 [00:10<00:18, 369.18it/s]\u001b[A\n"," 36% 3804/10570 [00:10<00:19, 351.63it/s]\u001b[A\n"," 36% 3842/10570 [00:10<00:18, 359.45it/s]\u001b[A\n"," 37% 3882/10570 [00:10<00:18, 369.12it/s]\u001b[A\n"," 37% 3920/10570 [00:10<00:18, 361.85it/s]\u001b[A\n"," 37% 3960/10570 [00:10<00:17, 370.52it/s]\u001b[A\n"," 38% 3998/10570 [00:10<00:17, 371.83it/s]\u001b[A\n"," 38% 4036/10570 [00:10<00:18, 361.32it/s]\u001b[A\n"," 39% 4073/10570 [00:10<00:17, 362.77it/s]\u001b[A\n"," 39% 4110/10570 [00:11<00:18, 358.78it/s]\u001b[A\n"," 39% 4146/10570 [00:11<00:19, 332.75it/s]\u001b[A\n"," 40% 4180/10570 [00:11<00:20, 312.15it/s]\u001b[A\n"," 40% 4212/10570 [00:11<00:22, 286.32it/s]\u001b[A\n"," 40% 4245/10570 [00:11<00:21, 296.36it/s]\u001b[A\n"," 40% 4276/10570 [00:11<00:23, 270.00it/s]\u001b[A\n"," 41% 4304/10570 [00:11<00:25, 246.04it/s]\u001b[A\n"," 41% 4340/10570 [00:11<00:22, 273.35it/s]\u001b[A\n"," 41% 4376/10570 [00:11<00:20, 294.97it/s]\u001b[A\n"," 42% 4411/10570 [00:12<00:19, 308.03it/s]\u001b[A\n"," 42% 4450/10570 [00:12<00:18, 330.59it/s]\u001b[A\n"," 42% 4491/10570 [00:12<00:17, 352.16it/s]\u001b[A\n"," 43% 4527/10570 [00:12<00:17, 340.67it/s]\u001b[A\n"," 43% 4566/10570 [00:12<00:16, 353.22it/s]\u001b[A\n"," 44% 4602/10570 [00:12<00:16, 353.83it/s]\u001b[A\n"," 44% 4638/10570 [00:12<00:16, 351.46it/s]\u001b[A\n"," 44% 4674/10570 [00:12<00:16, 350.45it/s]\u001b[A\n"," 45% 4715/10570 [00:12<00:16, 365.81it/s]\u001b[A\n"," 45% 4752/10570 [00:13<00:16, 347.59it/s]\u001b[A\n"," 45% 4788/10570 [00:13<00:16, 349.01it/s]\u001b[A\n"," 46% 4827/10570 [00:13<00:15, 359.88it/s]\u001b[A\n"," 46% 4864/10570 [00:13<00:16, 343.34it/s]\u001b[A\n"," 46% 4904/10570 [00:13<00:15, 359.07it/s]\u001b[A\n"," 47% 4943/10570 [00:13<00:15, 366.23it/s]\u001b[A\n"," 47% 4980/10570 [00:13<00:15, 356.42it/s]\u001b[A\n"," 47% 5017/10570 [00:13<00:15, 358.40it/s]\u001b[A\n"," 48% 5057/10570 [00:13<00:14, 368.70it/s]\u001b[A\n"," 48% 5094/10570 [00:14<00:15, 347.70it/s]\u001b[A\n"," 49% 5135/10570 [00:14<00:14, 363.89it/s]\u001b[A\n"," 49% 5172/10570 [00:14<00:14, 364.60it/s]\u001b[A\n"," 49% 5209/10570 [00:14<00:15, 352.10it/s]\u001b[A\n"," 50% 5248/10570 [00:14<00:14, 360.39it/s]\u001b[A\n"," 50% 5288/10570 [00:14<00:14, 371.12it/s]\u001b[A\n"," 50% 5326/10570 [00:14<00:14, 368.37it/s]\u001b[A\n"," 51% 5363/10570 [00:14<00:14, 368.02it/s]\u001b[A\n"," 51% 5403/10570 [00:14<00:13, 375.60it/s]\u001b[A\n"," 51% 5441/10570 [00:14<00:14, 365.59it/s]\u001b[A\n"," 52% 5478/10570 [00:15<00:14, 348.67it/s]\u001b[A\n"," 52% 5515/10570 [00:15<00:14, 352.68it/s]\u001b[A\n"," 53% 5551/10570 [00:15<00:14, 343.75it/s]\u001b[A\n"," 53% 5589/10570 [00:15<00:14, 351.37it/s]\u001b[A\n"," 53% 5626/10570 [00:15<00:13, 356.11it/s]\u001b[A\n"," 54% 5662/10570 [00:15<00:14, 345.93it/s]\u001b[A\n"," 54% 5702/10570 [00:15<00:13, 360.87it/s]\u001b[A\n"," 54% 5740/10570 [00:15<00:13, 366.13it/s]\u001b[A\n"," 55% 5777/10570 [00:15<00:13, 357.43it/s]\u001b[A\n"," 55% 5815/10570 [00:15<00:13, 362.56it/s]\u001b[A\n"," 55% 5852/10570 [00:16<00:13, 358.36it/s]\u001b[A\n"," 56% 5888/10570 [00:16<00:13, 354.09it/s]\u001b[A\n"," 56% 5926/10570 [00:16<00:12, 359.66it/s]\u001b[A\n"," 56% 5963/10570 [00:16<00:12, 362.62it/s]\u001b[A\n"," 57% 6000/10570 [00:16<00:13, 341.37it/s]\u001b[A\n"," 57% 6036/10570 [00:16<00:13, 346.62it/s]\u001b[A\n"," 57% 6072/10570 [00:16<00:12, 349.20it/s]\u001b[A\n"," 58% 6108/10570 [00:16<00:13, 341.92it/s]\u001b[A\n"," 58% 6148/10570 [00:16<00:12, 358.12it/s]\u001b[A\n"," 59% 6184/10570 [00:17<00:12, 355.24it/s]\u001b[A\n"," 59% 6221/10570 [00:17<00:12, 359.47it/s]\u001b[A\n"," 59% 6258/10570 [00:17<00:12, 351.52it/s]\u001b[A\n"," 60% 6299/10570 [00:17<00:11, 365.84it/s]\u001b[A\n"," 60% 6336/10570 [00:17<00:11, 357.26it/s]\u001b[A\n"," 60% 6374/10570 [00:17<00:11, 362.62it/s]\u001b[A\n"," 61% 6411/10570 [00:17<00:11, 356.85it/s]\u001b[A\n"," 61% 6448/10570 [00:17<00:11, 360.26it/s]\u001b[A\n"," 61% 6488/10570 [00:17<00:11, 370.16it/s]\u001b[A\n"," 62% 6526/10570 [00:17<00:10, 371.69it/s]\u001b[A\n"," 62% 6564/10570 [00:18<00:11, 357.90it/s]\u001b[A\n"," 62% 6602/10570 [00:18<00:10, 362.60it/s]\u001b[A\n"," 63% 6642/10570 [00:18<00:10, 370.83it/s]\u001b[A\n"," 63% 6680/10570 [00:18<00:10, 362.86it/s]\u001b[A\n"," 64% 6717/10570 [00:18<00:10, 362.73it/s]\u001b[A\n"," 64% 6756/10570 [00:18<00:10, 370.07it/s]\u001b[A\n"," 64% 6794/10570 [00:18<00:10, 359.97it/s]\u001b[A\n"," 65% 6831/10570 [00:18<00:10, 361.58it/s]\u001b[A\n"," 65% 6869/10570 [00:18<00:10, 365.19it/s]\u001b[A\n"," 65% 6906/10570 [00:19<00:10, 354.08it/s]\u001b[A\n"," 66% 6943/10570 [00:19<00:10, 357.53it/s]\u001b[A\n"," 66% 6982/10570 [00:19<00:09, 365.22it/s]\u001b[A\n"," 66% 7019/10570 [00:19<00:09, 362.43it/s]\u001b[A\n"," 67% 7058/10570 [00:19<00:09, 370.37it/s]\u001b[A\n"," 67% 7097/10570 [00:19<00:09, 374.56it/s]\u001b[A\n"," 68% 7137/10570 [00:19<00:09, 379.15it/s]\u001b[A\n"," 68% 7175/10570 [00:19<00:09, 373.06it/s]\u001b[A\n"," 68% 7213/10570 [00:19<00:09, 368.81it/s]\u001b[A\n"," 69% 7250/10570 [00:19<00:09, 361.89it/s]\u001b[A\n"," 69% 7287/10570 [00:20<00:09, 360.14it/s]\u001b[A\n"," 69% 7324/10570 [00:20<00:09, 357.21it/s]\u001b[A\n"," 70% 7360/10570 [00:20<00:09, 348.95it/s]\u001b[A\n"," 70% 7398/10570 [00:20<00:08, 356.57it/s]\u001b[A\n"," 70% 7437/10570 [00:20<00:08, 365.63it/s]\u001b[A\n"," 71% 7474/10570 [00:20<00:08, 359.74it/s]\u001b[A\n"," 71% 7513/10570 [00:20<00:08, 367.98it/s]\u001b[A\n"," 71% 7552/10570 [00:20<00:08, 371.79it/s]\u001b[A\n"," 72% 7590/10570 [00:20<00:08, 365.59it/s]\u001b[A\n"," 72% 7629/10570 [00:21<00:07, 372.29it/s]\u001b[A\n"," 73% 7668/10570 [00:21<00:07, 374.45it/s]\u001b[A\n"," 73% 7706/10570 [00:21<00:07, 362.99it/s]\u001b[A\n"," 73% 7744/10570 [00:21<00:07, 367.19it/s]\u001b[A\n"," 74% 7781/10570 [00:21<00:07, 367.04it/s]\u001b[A\n"," 74% 7818/10570 [00:21<00:07, 360.05it/s]\u001b[A\n"," 74% 7855/10570 [00:21<00:07, 361.47it/s]\u001b[A\n"," 75% 7895/10570 [00:21<00:07, 370.30it/s]\u001b[A\n"," 75% 7933/10570 [00:21<00:07, 370.97it/s]\u001b[A\n"," 75% 7971/10570 [00:21<00:07, 370.77it/s]\u001b[A\n"," 76% 8010/10570 [00:22<00:06, 373.83it/s]\u001b[A\n"," 76% 8048/10570 [00:22<00:07, 355.21it/s]\u001b[A\n"," 76% 8086/10570 [00:22<00:06, 361.23it/s]\u001b[A\n"," 77% 8127/10570 [00:22<00:06, 373.30it/s]\u001b[A\n"," 77% 8165/10570 [00:22<00:06, 358.71it/s]\u001b[A\n"," 78% 8202/10570 [00:22<00:06, 359.66it/s]\u001b[A\n"," 78% 8239/10570 [00:22<00:06, 361.55it/s]\u001b[A\n"," 78% 8276/10570 [00:22<00:06, 348.41it/s]\u001b[A\n"," 79% 8314/10570 [00:22<00:06, 357.29it/s]\u001b[A\n"," 79% 8353/10570 [00:23<00:06, 365.20it/s]\u001b[A\n"," 79% 8390/10570 [00:23<00:06, 351.16it/s]\u001b[A\n"," 80% 8427/10570 [00:23<00:06, 355.50it/s]\u001b[A\n"," 80% 8468/10570 [00:23<00:05, 371.06it/s]\u001b[A\n"," 80% 8506/10570 [00:23<00:05, 360.00it/s]\u001b[A\n"," 81% 8546/10570 [00:23<00:05, 370.58it/s]\u001b[A\n"," 81% 8585/10570 [00:23<00:05, 373.28it/s]\u001b[A\n"," 82% 8623/10570 [00:23<00:05, 363.55it/s]\u001b[A\n"," 82% 8661/10570 [00:23<00:05, 366.55it/s]\u001b[A\n"," 82% 8702/10570 [00:23<00:04, 377.31it/s]\u001b[A\n"," 83% 8740/10570 [00:24<00:05, 364.21it/s]\u001b[A\n"," 83% 8777/10570 [00:24<00:04, 364.09it/s]\u001b[A\n"," 83% 8814/10570 [00:24<00:04, 362.86it/s]\u001b[A\n"," 84% 8851/10570 [00:24<00:04, 353.45it/s]\u001b[A\n"," 84% 8891/10570 [00:24<00:04, 364.42it/s]\u001b[A\n"," 84% 8928/10570 [00:24<00:04, 363.19it/s]\u001b[A\n"," 85% 8965/10570 [00:24<00:04, 362.24it/s]\u001b[A\n"," 85% 9004/10570 [00:24<00:04, 369.50it/s]\u001b[A\n"," 86% 9041/10570 [00:24<00:04, 354.82it/s]\u001b[A\n"," 86% 9080/10570 [00:25<00:04, 362.92it/s]\u001b[A\n"," 86% 9118/10570 [00:25<00:03, 366.25it/s]\u001b[A\n"," 87% 9155/10570 [00:25<00:04, 348.83it/s]\u001b[A\n"," 87% 9194/10570 [00:25<00:03, 359.01it/s]\u001b[A\n"," 87% 9234/10570 [00:25<00:03, 368.56it/s]\u001b[A\n"," 88% 9272/10570 [00:25<00:03, 366.71it/s]\u001b[A\n"," 88% 9310/10570 [00:25<00:03, 370.22it/s]\u001b[A\n"," 88% 9348/10570 [00:25<00:03, 367.84it/s]\u001b[A\n"," 89% 9385/10570 [00:25<00:03, 361.28it/s]\u001b[A\n"," 89% 9424/10570 [00:25<00:03, 367.06it/s]\u001b[A\n"," 90% 9465/10570 [00:26<00:02, 378.59it/s]\u001b[A\n"," 90% 9503/10570 [00:26<00:02, 370.24it/s]\u001b[A\n"," 90% 9541/10570 [00:26<00:02, 367.06it/s]\u001b[A\n"," 91% 9581/10570 [00:26<00:02, 373.79it/s]\u001b[A\n"," 91% 9619/10570 [00:26<00:02, 366.90it/s]\u001b[A\n"," 91% 9658/10570 [00:26<00:02, 373.30it/s]\u001b[A\n"," 92% 9697/10570 [00:26<00:02, 376.27it/s]\u001b[A\n"," 92% 9735/10570 [00:26<00:02, 372.13it/s]\u001b[A\n"," 92% 9774/10570 [00:26<00:02, 376.61it/s]\u001b[A\n"," 93% 9812/10570 [00:26<00:02, 377.12it/s]\u001b[A\n"," 93% 9850/10570 [00:27<00:01, 372.88it/s]\u001b[A\n"," 94% 9888/10570 [00:27<00:01, 366.30it/s]\u001b[A\n"," 94% 9925/10570 [00:27<00:01, 364.67it/s]\u001b[A\n"," 94% 9962/10570 [00:27<00:01, 361.92it/s]\u001b[A\n"," 95% 9999/10570 [00:27<00:01, 361.54it/s]\u001b[A\n"," 95% 10039/10570 [00:27<00:01, 372.72it/s]\u001b[A\n"," 95% 10077/10570 [00:27<00:01, 361.15it/s]\u001b[A\n"," 96% 10116/10570 [00:27<00:01, 366.80it/s]\u001b[A\n"," 96% 10156/10570 [00:27<00:01, 374.21it/s]\u001b[A\n"," 96% 10194/10570 [00:28<00:01, 367.56it/s]\u001b[A\n"," 97% 10233/10570 [00:28<00:00, 373.53it/s]\u001b[A\n"," 97% 10272/10570 [00:28<00:00, 376.97it/s]\u001b[A\n"," 98% 10310/10570 [00:28<00:00, 359.36it/s]\u001b[A\n"," 98% 10350/10570 [00:28<00:00, 368.21it/s]\u001b[A\n"," 98% 10388/10570 [00:28<00:00, 371.30it/s]\u001b[A\n"," 99% 10426/10570 [00:28<00:00, 352.14it/s]\u001b[A\n"," 99% 10464/10570 [00:28<00:00, 358.93it/s]\u001b[A\n"," 99% 10503/10570 [00:28<00:00, 365.57it/s]\u001b[A\n","100% 10570/10570 [00:29<00:00, 363.81it/s]\n","08/02/2022 19:42:58 - INFO - utils_qa - Saving predictions to ./montura/MyDrive/TFG/minibert/mini_model/eval_predictions.json.\n","08/02/2022 19:42:58 - INFO - utils_qa - Saving nbest_preds to ./montura/MyDrive/TFG/minibert/mini_model/eval_nbest_predictions.json.\n","08/02/2022 19:43:03 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n","100% 1329/1329 [00:57<00:00, 22.94it/s] \n","***** eval metrics *****\n","  epoch            =     3.0\n","  eval_exact_match = 37.2753\n","  eval_f1          = 49.6881\n","  eval_samples     =   10626\n","[INFO|modelcard.py:468] 2022-08-02 19:43:03,461 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad', 'type': 'squad', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"]}]}]}